Goal
Leverage the city of Chicago’s rideshare dataset to visualize rideshare traffic in Chicago. 
Data
The data, found at data.cityofchicago.org, contains contains information on every rideshare trip since November 1st, 2018. It is inclusive of all rideshare companies, not just Uber and Lyft. In its entirety, the dataset consists of 21 columns of information on 45.2 million rides (11.2 GB). 


For our project, we were only concerned with 5 columns: Trip Start Timestamp, Pickup Centroid Latitude, Pickup Centroid Longitude, Pickup Centroid Latitude and Pickup Centroid Longitude. Trips that either started outside of the city or ended  outside of the city, had geo-coordinates of n/a for the respective fields. These rides were dropped from our analysis. After all of our cleaning heuristics, we were left with five columns of information on 39 million rides (3 GB).


For the security of the users exact pickup and dropoff geo coordinates were not provided, but rather the census tract geocoordinates. Chicago is split into 866 census tracts, ranging from 89,000 square feet to about 8-square miles. Therefore, at best, we are able to get within 89,000 square feet of any pickup/dropoff. In addition to obscuring the location of the rides, the timestamps were rounded to the nearest 15 minutes. 
Hypothesis
Using pickup and dropoff geo-coordinates, we represent trips as line segments. Our theory is that if two trips with identical trip start time stamp intersect, then there is traffic at that intersection. In this fashion we can compare each trip to all other trips, mark intersections on a map and thus visualize rideshare traffic. We hypothesize traffic will be heaviest during commuting hours (6-10AM and 4-8PM) and centered around the loop.
Algorithms


We originally used the sympy library to check for intersections between line segments. However, we found this to be more computationally complex than our line-sweep implementation. For instance, when analyzing our sample dataset of 10,000 rides, the line sweep implementation took 4 seconds while the sympy implementation took 1 minute and 45 seconds. This test was performed without using pandas to generate random points within a census tract, rather it used the epsilon method described below.
The line sweep algorithm works by having an imaginary line move across a plane. We keep track of the line segments that intersect the imaginary as it sweeps, the active segments. We check for intersections among the segments when the sweeping line intersects with the start or end point of a line segment. In our code, the line sweeps from left to right. When the vertical line, the sweeping line, first intersects with the left point of a line segment, we check if this new segment intersects with the active segments directly above and below it and add the new segment to the list of active segments. When the sweeping line intersects with the right point of a line segment, we remove it from the list of active segments and check if the two newly adjacent line segments intersect. We order active line segments by the y coordinate of the left point of the segment. By sweeping the line across all segments, we find all intersections.
To implement this algorithm, we need for there to be no vertical line segments, no two segments to intersect at their endpoints, no three or more segments to intersect at the same point, and no overlapping segments. Our data only gave us end points as the centroids of the census tract of dropoff and pickup, which is clearly not conducive to this algorithm. The algorithm worked to an extent for small samples, since there are 800 census tracts, but change was necessary to run it on the entire dataset. Initially we added a small epsilon type value to the x and y values of a point, to make the start and end points more random. This allows for the algorithm to work, but points scattered around the census tract centroid, not giving particularly accurate results. Later, we used the census tract id to generate a random point, which is approximately uniformly distributed, within the census tract. Using chicago’s census tract boundary dataset and the shapely library, we generated a x and y value randomly from a uniform distribution defined by the x and y bounds of the polygon representing the census tract, then checked if that point was inside the polygon with the shapely library. Both of these solutions allowed us to implement our algorithm, the ladder giving us more accurate results.
In addition, we need to order all points from left to right, keep track of whether a point is the left or right end of a segment, and keep track of active segments in a way that allows us to organize segments by their y coordinate and easily look up preceding and proceeding nodes. 
First, we organized each line segment such that its leftmost coordinate was first. Then we used pythons heappush on a tuple of (point.x, isLeft, segment), containing the x coordinate of a point, a boolean representing whether the point if the left or right point of a segment, and the segment itself. Using heappush on every point created a list of sorted points with all the information needed for our algorithm. Lastly, we used the bintrees library’s AVLTree, a self balancing binary tree,  to keep track of active segments. We used the y coordinate of the left point as the key and the corresponding line segment as the value. The AVLTree allows us to efficiently look up line segments and line segments adjacent to a given y coordinate, in turn allowing for our algorithm to be implemented.
        Lastly, we had to find the intersection between two line segments. To do this, we found the intersection between the two lines corresponding to the two segments, essentially by setting their equations equal to each other. Then, we checked if this point was in between the maximum left x value and the minimum right x value; if the point is, the lines intersect. We iterate through points, ordered by the heap, comparing them as described by the algorithm, storing active segments in the avl tree, allowing us to find all intersections of any amount of line segments. We used the mapper to sort the data by weekday and the time interval in which it started, the combiner to create a heap containing points, line segments and the isLeft boolean, and calculated intersections in the reducer. This approach is conducive to map reduce and was much faster than brute force checking for intersections with the sympy library. 


Challenges
        We used several try catch statements to make up for two shortcomings in our code. Firstly, the AVLTree we used returns a key error when we search for an node’s neighbor, and it does not have a neighbor in that direction. Instead of trying to find another data structure or modifying the AVLTree code, we simply used try except clauses to catch key errors and ignore them; if a neighbor does not exist, we simply do not check for intersections. Similarly, not all census tract ids in the rideshare data are in the chicago census tract boundary database. Again, we used a try catch clause to simply ignore any key errors and did not consider routes with census tracts not cataloged by the boundary database.
        There are several ways we could have made our code more efficient. We append --file geo.csv to our command in the google VM to allow us to use our geo.csv file, but mrjob’s built in file upload is recommended in their documentation. Their documentation is not clear on out how to use this method both with and without dataproc. We were not clear what, if any different, this implementation would make, and were short on time, it was not implemented. Similarly, querying a SQL database to receive the string used by shapely to generate points would likely be much more efficient than creating a pandas dataframe in each combiner. Since we are not experienced with SQL and already were having difficulty with dataproc, time prevented us from implementing this more efficient approach. 


Results
Our results are from a random sample of 100,000,000 rides. The results are elegantly displayed as folium heataps. These maps reside in our repository as html files. One map serves as a visualization of traffic by day of the week, the other as traffic by hour. 


As expected, traffic is centered around the loop. There is also a significant amount of traffic surrounding Chicago’s airports. Hourly, the airports experience the most traffic around 6-7AM. Overall, the heaviest traffic occurs during commuter hours. The traffic dynamic evidently change from day to day. On Sunday, there is significant traffic surrounding O’Hare.